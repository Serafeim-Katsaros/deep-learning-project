ΕΡΓΑΣΊΑ 4
Όνομα: Σεραφείμ Κατσαρός
ΑΕΜ: 4565 

1.Hyper-parameter Tuning:
Συνάρτηση ενεργοποίησης	Αριθμός Επιπέδων	Πλήθος νευρώνων ανά επίπεδο	Ρυθμός μάθησης	Εποχές	Ακρίβεια
tanh	                        3	                256	                       0.001	 10	    93,18%
tanh	                        2	                128	                       0.001	 10	    92,89%
relu	                        2	                256	                       0.001	 10	    95,39%
relu	                        3	                256	                       0.001	 10	    95,44%
relu	                        2	                256                        0.001	 15	    95,79%
relu	                        2	                256	                       0.01	     10	    11,35%


tanh vs relu: Η ReLU γενικά δίνει καλύτερη απόδοση γιατί αποφεύγει το πρόβλημα εξαφάνισης βαθμού και επιταχύνει τη μάθηση. Η tanh δουλεύει καλά αλλά έχει μικρότερη ακρίβεια.

Αριθμός επιπέδων και νευρώνων: Περισσότερα επίπεδα και νευρώνες αυξάνουν τη χωρητικότητα του μοντέλου, βελτιώνοντας ελαφρώς την ακρίβεια, αλλά μετά από κάποιο σημείο οι βελτιώσεις είναι μικρές.

Ρυθμός μάθησης: Ο ρυθμός 0.001 είναι κατάλληλος για σταθερή και σωστή εκπαίδευση. Ο πολύ μεγάλος ρυθμός (0.01) προκαλεί αστάθεια και χαμηλή ακρίβεια.

Εποχές: Περισσότερες εποχές επιτρέπουν καλύτερη εκμάθηση και βελτιώνουν την ακρίβεια, όπως φαίνεται με την αύξηση από 10 σε 15.

Συνολικά, η καλύτερη απόδοση επιτεύχθηκε με ReLU, 2 επίπεδα, 256 νευρώνες, ρυθμό μάθησης 0.001 και 15 εποχές.

2. Improvements: 
Το Batch Normalization (BN) είναι μια τεχνική που "κανονικοποιεί" (normalize) τις εισόδους κάθε layer κατά τη διάρκεια της εκπαίδευσης, έτσι ώστε να έχουν μέση τιμή κοντά στο 0 και τυπική απόκλιση κοντά στο 1.

model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(784,), name='input'),
    tf.keras.layers.Dense(256, name='hidden-1'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Dense(256, name='hidden-2'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation('relu'),
tf.keras.layers.Dense(10, activation='softmax', name='outputs')

Το Batch Normalization βοηθά στην σταθερότητα και καλύτερη γενίκευση.
Η μικρότερη accuracy στο training δεν είναι απαραίτητα κακό, καθώς δείχνει ότι το μοντέλο δεν υπερεκπαιδεύεται.
Η validation accuracy με BN είναι σχεδόν ίδια ή λίγο καλύτερη (λιγότερο "θορυβώδης") από αυτή χωρίς BN.
Χωρίς BN, μπορεί να φτάνεις σε μεγαλύτερη training accuracy αλλά με κίνδυνο υπερπροσαρμογής (overfitting).

3.Ερωτήσεις Κατανόησης:
A.Ναι, η MNIST είναι ένα καλό και δημοφιλές dataset για εκπαίδευση και δοκιμή μοντέλων αναγνώρισης χειρόγραφων ψηφίων, λόγω του μεγάλου μεγέθους του (60.000 δείγματα εκπαίδευσης) και της καλής ποιότητας των εικόνων.
B.Όχι, δεν είναι όλα τα pixel σημαντικά. Πολλά pixel μπορεί να περιέχουν θόρυβο ή κενό φόντο που δεν επηρεάζει την αναγνώριση.
C.  Μεγάλα και πολύπλοκα δεδομένα, όπως εικόνες, ήχο, κείμενο.
Προβλήματα όπου απαιτείται αυτόματη εξαγωγή χαρακτηριστικών (feature extraction).
Απαιτήσεις για υψηλή ακρίβεια και δυνατότητα εκμάθησης σύνθετων σχέσεων μέσα στα δεδομένα.
Εφαρμογές σε Computer Vision, Φωνητική Αναγνώριση
D.Supervised Learning: Εκπαίδευση με ετικέτες (π.χ. ταξινόμηση εικόνων).
Unsupervised Learning: Χωρίς ετικέτες, π.χ. αυτόματη ομαδοποίηση (clustering), μείωση διαστάσεων.
Reinforcement Learning: Εκπαίδευση μέσω αλληλεπίδρασης με το περιβάλλον για βελτιστοποίηση μιας πολιτικής δράσης
